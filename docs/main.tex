%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}

\usepackage{listings}
\usepackage{xcolor}

% Definição de cores personalizadas
\definecolor{mygreen}{rgb}{0,0.6,0}        % Para comentários
\definecolor{mygray}{rgb}{0.5,0.5,0.5}       % Para números de linha
\definecolor{mymauve}{rgb}{0.58,0,0.82}       % Para strings
\definecolor{myblue}{rgb}{0,0,0.8}            % Para palavras-chave
\definecolor{lightgray}{rgb}{0.95,0.95,0.95}  % Fundo dos códigos

% Configuração padrão para todos os códigos
\lstset{
  backgroundcolor=\color{lightgray},
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  captionpos=b,
  numbers=left,
  numberstyle=\tiny\color{mygray},
  frame=single,
  keywordstyle=\color{myblue}\bfseries,
  commentstyle=\color{mygreen}\itshape,
  stringstyle=\color{mymauve},
  tabsize=4,
  showstringspaces=false,
  aboveskip=10pt,
  belowskip=5pt
}
% Estilo para código em C
\lstdefinestyle{CStyle}{
  language=C,
  % Outras opções específicas para C podem ser adicionadas aqui
}

% Definindo uma linguagem para CUDA baseada em C++
\lstdefinelanguage{CUDA}[]{C++}{
  morekeywords={__global__, __device__, __shared__, __host__},
  % Caso necessário, adicione mais palavras-chave ou opções específicas
}

% Estilo para código em Python
\lstdefinestyle{PythonStyle}{
  language=Python,
  morekeywords=[2]{SequentialDiffusionEquation,with,OMPdiffusionEquation,CUDADiffusionEquation},
  % Outras opções específicas para Python podem ser adicionadas aqui
}

\renewcommand{\lstlistingname}{Código}

\sloppy

\title{Engane a IA: Desenvolvendo Perturbações Adversariais Contra Classificadores de DeepFake}

\author{Eduardo Verissimo Faccio, Guilherme Ferreira Lourenço}

\address{Instituto de Ciência e Tecnologia -- Universidade Federal de São Paulo
  (UNIFESP)\\
  São José dos Campos -- SP -- Brasil
  \email{\{verissimo.eduardo,gflourenco\}@unifesp.br}
}

\begin{document}

\maketitle

\begin{resumo}
  Este trabalho explora a vulnerabilidade e a robustez de redes neurais diante de ataques adversariais
  imperceptíveis aos humanos. Utilizando a base de dados “140k Real and Fake Faces”, treinou-se uma rede
  neural convolucional, uma mini UNET e um XGBoost para a classificação de imagens faciais, atingindo alta
  acurácia em condições normais. Em seguida, desenvolveu-se um ataque adversarial baseado em uma arquitetura
  UNET modificada, capaz de gerar perturbações sutis que comprometem significativamente a performance do classificador.
  Os resultados demonstram que, mesmo com perturbações invisíveis a olho nu, a confiabilidade dos sistemas de reconhecimento
  facial pode ser severamente comprometida, ressaltando a importância de desenvolver estratégias para aumentar
  a resiliência dos classificadores.

  \textbf{Palavras-chave}: Ataques adversariais, robustez, redes neurais convolucionais, perturbações imperceptíveis, reconhecimento facial, XGBoost.
\end{resumo}

\section{Introdução}

O avanço das técnicas de aprendizado profundo tem permitido que sistemas de
reconhecimento facial alcancem níveis de acurácia sem precedentes, contribuindo
para uma ampla gama de aplicações, desde segurança pública até interfaces de
usuário baseadas em biometria~\cite{parkhi2015deep}. Entretanto, apesar do
desempenho elevado, diversos estudos demonstraram que redes neurais são
inerentemente vulneráveis a perturbações adversariais - pequenas modificações
imperceptíveis a olho nu que podem levar a classificações
equivocadas~\cite{szegedy2014intriguingpropertiesneuralnetworks,goodfellow2015explainingharnessingadversarialexamples}.

Neste contexto, o presente trabalho investiga a robustez de modelos de
classificação de imagens faciais quando expostos a ataques adversariais sutis.
Utilizando a base de dados ``140k Real and Fake Faces'', foram treinados três
classificadores distintos - uma rede neural convolucional (CNN), uma versão
reduzida de UNET e um classificador baseado em XGBoost \cite{Chen_2016}
- que, sob condições normais, alcançaram alta performance. Em seguida,
propõe-se um ataque adversarial que se baseia em uma arquitetura UNET
modificada, cujo objetivo é gerar perturbações imperceptíveis que comprometam a
confiabilidade dos classificadores.

Ao integrar modelos de naturezas distintas na análise, este estudo busca
identificar possíveis diferenças na robustez dos sistemas frente a ataques
adversariais, evidenciando as limitações dos métodos de classificação atuais e
a necessidade de desenvolver estratégias de defesa que aumentem a resiliência
dos sistemas de reconhecimento facial. A abordagem proposta não só contribui
para uma compreensão mais aprofundada das vulnerabilidades dos modelos de
aprendizado profundo, mas também sugere caminhos para a implementação de
mecanismos de defesa que possam mitigar os impactos desses ataques em
aplicações críticas.

\section{Conclusão}

\bibliographystyle{sbc}
\bibliography{sbc-template}
\nocite{*}
\end{document}
